{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Write out the equation for trigram probability estimation (modifying Eq. 3.11 from SLP Chapter 3). Now write out all the non-zero trigram probabilities for the I am Sam corpus in Chapter 3 on page 4.**\n",
    "\n",
    "$$P(w_i | w_{i-2}, w_{i-1}) = \\frac{c(w_{i-2}, w_{i-1},w_{i})}{c(w_{i-2}, w_{i-1})}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Write a program to compute unsmoothed n-grams. Use the Dr. Seuss corpus to test this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Sam',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'Sam',\n",
       " 'I',\n",
       " 'am',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'like',\n",
       " 'green',\n",
       " 'eggs',\n",
       " 'and',\n",
       " 'ham',\n",
       " '</s>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_process_corpus(corpus):\n",
    "    return re.findall(r'[A-Za-z|(<s>)|(</s>)]+', corpus)\n",
    "\n",
    "corpus = '''\n",
    "<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I do not like green eggs and ham </s>\n",
    "'''\n",
    "corpus = pre_process_corpus(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[(None, 2, 3), (None, 3, 4), (None, 4, 5), (None, 5, 6), (None, 6, 7), (None, 7, 8), (None, 8, 9), (None, 9, None)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import tee, islice, zip_longest\n",
    "\n",
    "def get_consecutive_triples(iterable):\n",
    "    a, b, c = tee(iterable, 3)\n",
    "    next(b, None)\n",
    "    next(c, None)\n",
    "    next(c, None)\n",
    "    return zip_longest(a, b, c)\n",
    "\n",
    "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "triples = list(get_consecutive_triples(my_list))\n",
    "print(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_n_grams(corpus, n = 2):\n",
    "    # suppose corpus is a list of tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Write a definition of perplexity in language modeling.**\n",
    "\n",
    "**perplexity** is a metric to measure the performance of different natual language models in an intrinsic way. Or in other words, without taking into account any particular application.\n",
    "\n",
    "The metric is inversaly proporational to the probablitity of a sentence, and normalized by the number of words (N).\n",
    "\n",
    "It's helpful for quick prototyping models (and choosing between them), but it doesn't guarantee to get an extrinsic result in a specific applicaiton.\n",
    "\n",
    "It can also be understand in terms of information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Add an option to your program to compute the perplexity of a test set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. You are given a training set of 100 numbers that consists of 91 zeros and 1 each of\n",
    "the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is\n",
    "the unigram perplexity?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
