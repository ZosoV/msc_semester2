{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1 Naive Bayes Classification and Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Train two models, multinomial naive Bayes and binarized naive Bayes, both with\n",
    "add-1 smoothing, on the following document counts for key sentiment words, with\n",
    "positive or negative class assigned as noted.**\n",
    "\n",
    "```\n",
    "doc ”good” ”poor” ”great” (class)\n",
    "d1. 3 0 3 pos\n",
    "d2. 0 1 2 pos\n",
    "d3. 1 3 0 neg\n",
    "d4. 1 5 2 neg\n",
    "d5. 0 2 0 neg\n",
    "```\n",
    "\n",
    "**Use both naive Bayes models to assign a class (pos or neg) to this sentence:\n",
    "A good, good plot and great characters, but poor acting.\n",
    "Do the two models agree or disagree?**\n",
    "\n",
    "They disagree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.149946861188155\n",
      "-5.598421958998374\n",
      "1 pos\n"
     ]
    }
   ],
   "source": [
    "# Add-one smoothing\n",
    "log_p_neg_sentence = np.log(3/5) + np.log(3/17) + np.log(3/17) + np.log(3/17) + np.log(11/17)\n",
    "log_p_pos_sentence = np.log(2/5) + np.log(4/12) + np.log(4/12) + np.log(6/12) + np.log(2/12)\n",
    "\n",
    "\n",
    "print(log_p_neg_sentence)\n",
    "print(log_p_pos_sentence)\n",
    "\n",
    "classes = [\"neg\", \"pos\"]\n",
    "\n",
    "\n",
    "estimated_class = np.argmax(np.array([log_p_neg_sentence, log_p_pos_sentence]))\n",
    "print(estimated_class,  classes[estimated_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.023057814094814\n",
      "-6.056003068245553\n",
      "0 neg\n"
     ]
    }
   ],
   "source": [
    "# Binaryzed Naive Bayes\n",
    "\n",
    "log_p_neg_sentence = np.log(3/5) + np.log(3/9) + np.log(3/9) + np.log(2/9) + np.log(4/9)\n",
    "log_p_pos_sentence = np.log(2/5) + np.log(2/8) + np.log(2/8) + np.log(3/8) + np.log(2/8)\n",
    "\n",
    "\n",
    "print(log_p_neg_sentence)\n",
    "print(log_p_pos_sentence)\n",
    "\n",
    "classes = [\"neg\", \"pos\"]\n",
    "\n",
    "\n",
    "estimated_class = np.argmax(np.array([log_p_neg_sentence, log_p_pos_sentence]))\n",
    "print(estimated_class,  classes[estimated_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider an NLP classification task where a model is trained to classify text docu\n",
    "ments into two categories: pos and neg. After testing the model on a validation set,\n",
    "the following confusion matrix was obtained:**\n",
    "\n",
    "```\n",
    "Actual / Predicted      pos neg\n",
    "                    pos 80 20\n",
    "                    neg 30 70\n",
    "```\n",
    "\n",
    "**Based on the confusion matrix provided above, calculate the accuracy, and the pre-\n",
    "cision, recall and F1 for the pos class. Provide your answers in decimal form rounded\n",
    "to two decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.75\n",
      "precision:  0.8\n",
      "recall:  0.7272727272727273\n",
      "f1:  0.761904761904762\n"
     ]
    }
   ],
   "source": [
    "accuracy = (80+70)/200\n",
    "\n",
    "precision = 80/(80+20)\n",
    "\n",
    "recall = 80/(80+30)\n",
    "\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"f1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.75\n",
      "precision:  0.8\n",
      "recall:  0.7272727272727273\n",
      "f1:  0.761904761904762 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.75, 0.8, 0.7272727272727273, 0.761904761904762)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(confusion_matrix):\n",
    "    # Calculate true positives, false positives, and false negatives for each class\n",
    "    accuracy = (confusion_matrix[0,0] + confusion_matrix[1,1]) / np.sum(confusion_matrix)\n",
    "\n",
    "    precision = confusion_matrix[0,0] / (confusion_matrix[0,0] + confusion_matrix[0,1])\n",
    "\n",
    "    recall = confusion_matrix[0,0] / (confusion_matrix[0,0] + confusion_matrix[1,0])\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1_score, \"\\n\")\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "calculate_metrics(np.array([[80, 20], [30, 70]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. In another NLP task, a model is developed to classify text documents into three\n",
    "categories: Positive (pos), Neutral (neut), and Negative (neg). After deploying the\n",
    "model on a test dataset, the following confusion matrix was recorded:\n",
    "Using the confusion matrix above, calculate the following metrics: accuracy, and\n",
    "precision, recall, F1 for each class. Then calculate the micro-average precision, recall**\n",
    "\n",
    "```\n",
    "Actual / Predicted      pos neut neg\n",
    "                    pos  100  20 10\n",
    "                    neut 330 120 20\n",
    "                    neg   15  25 95\n",
    "```\n",
    "\n",
    "and F1. Finally calculate the macro-average precision, recall and F1 for the classifier.\n",
    "Provide your answers in decimal form rounded to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confussion_matrix_class_0:  \n",
      " [[100  30]\n",
      " [345 260]]\n",
      "confussion_matrix_class_1:  \n",
      " [[120 350]\n",
      " [ 45 220]]\n",
      "confussion_matrix_class_2:  \n",
      " [[ 95  40]\n",
      " [ 30 570]]\n",
      "metric for class pos\n",
      "accuracy:  0.4897959183673469\n",
      "precision:  0.7692307692307693\n",
      "recall:  0.2247191011235955\n",
      "f1:  0.34782608695652173 \n",
      "\n",
      "metric for class neu\n",
      "accuracy:  0.46258503401360546\n",
      "precision:  0.2553191489361702\n",
      "recall:  0.7272727272727273\n",
      "f1:  0.3779527559055118 \n",
      "\n",
      "metric for class neg\n",
      "accuracy:  0.9047619047619048\n",
      "precision:  0.7037037037037037\n",
      "recall:  0.76\n",
      "f1:  0.7307692307692308 \n",
      "\n",
      "macro_precision:  0.5760845406235477\n",
      "macro_recall:  0.5706639427987743\n",
      "macro_f1:  0.4855160245437548 \n",
      "\n",
      "accuracy:  0.6190476190476191\n",
      "precision:  0.42857142857142855\n",
      "recall:  0.42857142857142855\n",
      "f1:  0.42857142857142855 \n",
      "\n",
      "micro_accuracy:  0.6190476190476191\n",
      "micro_precision:  0.42857142857142855\n",
      "micro_recall:  0.42857142857142855\n",
      "micro_f1:  0.42857142857142855 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = np.array([\n",
    "    [100, 20, 10],\n",
    "    [330, 120, 20],\n",
    "    [15, 25, 95]\n",
    "])\n",
    "\n",
    "def get_confusion_matrix_class(confusion_matrix, class_index):\n",
    "\n",
    "    right_part = 0\n",
    "    for col in range(confusion_matrix.shape[1]):\n",
    "        if col != class_index:\n",
    "            right_part += confusion_matrix[class_index, col]\n",
    "\n",
    "    bottom_part = 0\n",
    "    for row in range(confusion_matrix.shape[0]):\n",
    "        if row != class_index:\n",
    "            bottom_part += confusion_matrix[row, class_index]\n",
    "\n",
    "    incliding_diagonal = 0\n",
    "    for row in range(confusion_matrix.shape[0]):\n",
    "        for col in range(confusion_matrix.shape[1]):\n",
    "            if row != class_index and col != class_index:\n",
    "                incliding_diagonal += confusion_matrix[row, col]\n",
    "    \n",
    "\n",
    "    return np.array([\n",
    "        [confusion_matrix[class_index, class_index], right_part],\n",
    "        [bottom_part, incliding_diagonal]\n",
    "    ])\n",
    "\n",
    "confusion_matrix_class_0 = get_confusion_matrix_class(confusion_matrix, 0)\n",
    "confusion_matrix_class_1 = get_confusion_matrix_class(confusion_matrix, 1)\n",
    "confusion_matrix_class_2 = get_confusion_matrix_class(confusion_matrix, 2)\n",
    "print(\"confussion_matrix_class_0: \", \"\\n\", confusion_matrix_class_0)\n",
    "print(\"confussion_matrix_class_1: \", \"\\n\", confusion_matrix_class_1)\n",
    "print(\"confussion_matrix_class_2: \", \"\\n\", confusion_matrix_class_2)\n",
    "\n",
    "print(\"metric for class pos\")\n",
    "accuracy_pos, precision_pos, recall_pos, f1_pos = calculate_metrics(confusion_matrix_class_0)\n",
    "print(\"metric for class neu\")\n",
    "accuracy_neu, precision_neu, recall_neu, f1_neu = calculate_metrics(confusion_matrix_class_1)\n",
    "\n",
    "print(\"metric for class neg\")\n",
    "accuracy_neg, precision_neg, recall_neg, f1_neg = calculate_metrics(confusion_matrix_class_2)\n",
    "\n",
    "macro_precision = (precision_pos + precision_neu + precision_neg) / 3\n",
    "macro_recall = (recall_pos + recall_neu + recall_neg) / 3\n",
    "macro_f1 = (f1_pos + f1_neu + f1_neg) / 3\n",
    "print(\"macro_precision: \", macro_precision)\n",
    "print(\"macro_recall: \", macro_recall)\n",
    "print(\"macro_f1: \", macro_f1, \"\\n\")\n",
    "\n",
    "pooled_confussion_matrix = confusion_matrix_class_0 + confusion_matrix_class_1 + confusion_matrix_class_2\n",
    "\n",
    "micro_accuracy, micro_precision, micro_recall, micro_f1 = calculate_metrics(pooled_confussion_matrix)\n",
    "\n",
    "print(\"micro_accuracy: \", micro_accuracy)\n",
    "print(\"micro_precision: \", micro_precision)\n",
    "print(\"micro_recall: \", micro_recall)\n",
    "print(\"micro_f1: \", micro_f1, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_pos:  0.7692307692307693\n",
      "recall_pos:  0.2247191011235955\n",
      "f1_pos:  0.34782608695652173\n",
      "precision_neu:  0.2553191489361702\n",
      "recall_neu:  0.7272727272727273\n",
      "f1_neu:  0.3779527559055118\n",
      "precision_neg:  0.7037037037037037\n",
      "recall_neg:  0.76\n",
      "f1_neg:  0.7307692307692308\n",
      "macro_precision:  0.5760845406235477\n",
      "macro_recall:  0.5706639427987743\n",
      "macro_f1:  0.4855160245437548\n"
     ]
    }
   ],
   "source": [
    "precision_pos = 100/130\n",
    "precision_neu = 120/470\n",
    "precision_neg = 95/135\n",
    "\n",
    "recall_pos = 100/445\n",
    "recall_neu = 120/165\n",
    "recall_neg = 95/125\n",
    "\n",
    "f1_pos = 2 * (precision_pos * recall_pos) / (precision_pos + recall_pos)#\n",
    "f1_neu = 2 * (precision_neu * recall_neu) / (precision_neu + recall_neu)\n",
    "f1_neg = 2 * (precision_neg * recall_neg) / (precision_neg + recall_neg)\n",
    "\n",
    "print(\"precision_pos: \", precision_pos)\n",
    "print(\"recall_pos: \", recall_pos)\n",
    "print(\"f1_pos: \", f1_pos)\n",
    "\n",
    "print(\"precision_neu: \", precision_neu)\n",
    "print(\"recall_neu: \", recall_neu)\n",
    "print(\"f1_neu: \", f1_neu)\n",
    "\n",
    "print(\"precision_neg: \", precision_neg)\n",
    "print(\"recall_neg: \", recall_neg)\n",
    "print(\"f1_neg: \", f1_neg)\n",
    "\n",
    "macro_precision = (precision_pos + precision_neu + precision_neg) / 3\n",
    "macro_recall = (recall_pos + recall_neu + recall_neg) / 3\n",
    "macro_f1 = (f1_pos + f1_neu + f1_neg) / 3\n",
    "print(\"macro_precision: \", macro_precision)\n",
    "print(\"macro_recall: \", macro_recall)\n",
    "print(\"macro_f1: \", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Briefly explain what the bootstrap method is and why it is appropriate for comparing\n",
    "two classifiers.**\n",
    "\n",
    "The boostrap method is a method to create artificial test data sets from an observed test set in order to perform an non-parametric hypothesis testing.\n",
    "\n",
    "It consists in creating artificial test data sets by sampling randomnly with replacement from a base (or reference) test data set. Under the assumption that the base sample is a representative of the population.\n",
    "\n",
    "It's appropiate for comparing two classifiers because bootstrap normaly work with paired tests, which compare two sets of observations that somehow are aligned. When we analyze two classifier models A and B, the performance of A and B is paired by comparing them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Describe the steps you would take to apply the bootstrap method to compare Model\n",
    "A and Model B.**\n",
    "\n",
    "1. Measure the performance of metric $M$ in a base test set (size N) for the model A and B, and display the results.\n",
    "2. Create artificial test results by ramdonly samplign with replacement from the results (of size N).\n",
    "    - It is practically pick randomly with replacement one of those N results.\n",
    "3. Repeat the process until complete a set of results of size N.\n",
    "4. Repeat the process b times until generate b artificial test results.\n",
    "5. One we have b artificial test sets, we can calculate our p-value as follows\n",
    "\n",
    "$$\\text{p-value}(x) = \\frac{1}{b}\\sum_{i=1}^{b}\\left(\\delta(x^{(i)}) - \\delta(x) \\geq \\delta(x) \\right)$$\n",
    "\n",
    "$$\\text{p-value}(x) = \\frac{1}{b}\\sum_{i=1}^{b}\\left(\\delta(x^{(i)}) \\geq 2\\delta(x) \\right)$$\n",
    "\n",
    "where $x$ is the base test set and $x^{(i)}$ are the artificial test sample from $x$, and\n",
    "\n",
    "$$\\delta(x) = M(A,x) - M(B,x)$$\n",
    "\n",
    "is the performance difference from A and B of metric $M$\n",
    "\n",
    "6. If $\\text{p-value} < \\alpha$, we can reject the null hypothesis $H_0: \\delta(x) \\leq 0$ and accept $H_1: \\delta(x) > 0$. In other words, accept that A is better than B wrt the performance on $M$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
